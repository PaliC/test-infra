# Week of 2024-06-03 to 2024-06-10 (31)

### GHFirst (11)

- [Revert "[RFC] Provide optional switches to _dump_nccl_trace (#127651)"](https://github.com/pytorch/pytorch/commit/02a901f1e9136f45d8993c5b4ec23031b3bf0bcf)
  - Breaks internal CI ([comment](https://github.com/pytorch/pytorch/pull/127651#issuecomment-2156076838))
- [Revert "[RFC] add per-collective timeout value in flight recorder (#128190)"](https://github.com/pytorch/pytorch/commit/57a24c4fdb58b82724b4f3d55d3af105a660bf39)
  - Sorry need to revert this, in conflict with https://github.com/pytorch/pytorch/pull/127651 that needs reverting ([comment](https://github.com/pytorch/pytorch/pull/128190#issuecomment-2156075318))
- [Revert "Complete revamp of float/promotion sympy handling (#126905)"](https://github.com/pytorch/pytorch/commit/ac51f782fe012af58af57bd5e8aab781ed07c90c)
  - Sorry need to revert - failing internally ([comment](https://github.com/pytorch/pytorch/pull/126905#issuecomment-2155118778))
- [Revert "[inductor] simplify indexing (#127661)"](https://github.com/pytorch/pytorch/commit/23c156cd2d699ea1f67deae2bf4353e327daf16b)
  - Sorry reverting because in conflict with https://github.com/pytorch/pytorch/pull/126905 which needs to be reverted, will be relanding it ([comment](https://github.com/pytorch/pytorch/pull/127661#issuecomment-2155115388))
- [Revert "Make ValueRange repr less chatty by default (#128043)"](https://github.com/pytorch/pytorch/commit/224b4339e590a6390e3e23fb05f11efbd4b3238a)
  - Sorry reverting because in conflict with [#126905](https://github.com/pytorch/pytorch/pull/126905) which needs to be reverted ([comment](https://github.com/pytorch/pytorch/pull/128043#issuecomment-2155091732))
- [Revert "FP8 rowwise scaling (#125204)"](https://github.com/pytorch/pytorch/commit/a5b86a1ec0150f78d88fc389df4909212ece0108)
  - Sorry need to revert this failing, on internal CI. I suggest to reimport this and try to land internally resolving all issues ([comment](https://github.com/pytorch/pytorch/pull/125204#issuecomment-2152905513))
- [Revert "[dynamo] Support ndarray.dtype attribute access (#124490)"](https://github.com/pytorch/pytorch/commit/48a54146e78773bac268493a6b4eb9be392b1b9e)
  - Breaks internal builds ([comment](https://github.com/pytorch/pytorch/pull/124490#issuecomment-2152664749))
- [Revert "[torchbind] always fakify script object by default in non-strict export (#127116)"](https://github.com/pytorch/pytorch/commit/4c074a9b8bd2e6d8940b40a41ce399e6c4a463a9)
  - Failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/127116#issuecomment-2147459339))
- [Revert "[Inductor] FlexAttention backward kernel optimization (#127208)"](https://github.com/pytorch/pytorch/commit/2fc907971a0ba22c6f6f65295a9d5e7fe501aca7)
  - test_flex_attention is failing internally ([comment](https://github.com/pytorch/pytorch/pull/127208#issuecomment-2145830810))
- [Revert "[Inductor] Add FlexAttention backward kernel dynamic shape tests (#127728)"](https://github.com/pytorch/pytorch/commit/3f45fa63f289d465de2694a7ec78ce90cd771b37)
  - Ineternal breakage of https://github.com/pytorch/pytorch/pull/127208 hence reverting ([comment](https://github.com/pytorch/pytorch/pull/127728#issuecomment-2145822667))
- [Revert "[Inductor][Flex-attention] Support different sequence lengths for Query and Key/Value (#127678)"](https://github.com/pytorch/pytorch/commit/c35b65715cac6b2ab9af8a3d1c4b223eca1d6f93)
  - Ineternal breakage of https://github.com/pytorch/pytorch/pull/127208 hence reverting ([comment](https://github.com/pytorch/pytorch/pull/127678#issuecomment-2145821489))

### Ignored Signal (3)

- [Revert "[dynamo] Bugfix for nn parameter construction (#127806)"](https://github.com/pytorch/pytorch/commit/6dc0a291b9bf27aa7258866591f20ed246acb81c)
  - causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))
- [Revert "Retire torch.distributed.pipeline (#127354)"](https://github.com/pytorch/pytorch/commit/0ff60236abfa6d60c4c9caf2f812f82f23530a49)
  - Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))
- [Revert "[BE]: Update mypy to 1.10.0 (#127717)"](https://github.com/pytorch/pytorch/commit/84776d7597c801fd23cdf8b8c320c633914b8bd4)
  - I am not sure why but the failures look legit and they are showing up in trunk https://hud.pytorch.org/pytorch/pytorch/commit/30213ab0a7b27277e76ea9dd707ce629a63d91ee ([comment](https://github.com/pytorch/pytorch/pull/127717#issuecomment-2144183347))

### Landrace (2)

- [Revert "Add OpInfo entry for alias_copy (#127232)"](https://github.com/pytorch/pytorch/commit/c58d3af3b47dd1413c1401fe9e1d90d00d428cd0)
  - broke [onnx](https://github.com/pytorch/pytorch/actions/runs/9397057801/job/25880181144) and [mps](https://github.com/pytorch/pytorch/actions/runs/9397057805/job/25879818705) tests, [hud link](https://hud.pytorch.org/pytorch/pytorch/commit/457df212e1c6e1aa4f1eb2ad6ee292052d7c07e1) , base is 15 days old, the onnx test xfailed on the pr but the xfail was removed so if you rebase itll surface, mps build failed so no mps tests were run on the pr ([comment](https://github.com/pytorch/pytorch/pull/127232#issuecomment-2152848758))
- [Revert "Add aten._unsafe_masked_index (#116491)"](https://github.com/pytorch/pytorch/commit/d1fad416a817b3360964f7d0ab4e448cac7ce367)
  - breaking onnx tests, test xfailed on this PR but I assume the xfail was removed on main ([comment](https://github.com/pytorch/pytorch/pull/116491#issuecomment-2145557724))

### Not through pytorchbot (3)

- [Revert "Default XLA to use swap_tensors path in nn.Module._apply (#126814)" (#128170)](https://github.com/pytorch/pytorch/commit/65aa16f968af2cd18ff8c25cc657e7abda594bfc)
- [Revert "Inductor respects strides for custom ops by default (#126986)" (#127923)](https://github.com/pytorch/pytorch/commit/0eb9ec958a949baf1733248cdb2ac36d22fe8c1f)
- [Revert "correct BLAS input (#126200)" (#127762)](https://github.com/pytorch/pytorch/commit/53f001c5993c4fd3cf99eaaae03012fb5e99c18e)

### No Signal (9)

- [Revert "[dynamo][nn-modules] Trace through nn.Module dunder methods for UnspecializedNNModule (#126578)"](https://github.com/pytorch/pytorch/commit/44371bd43276b27aa5da0e223fb7daaf52558767)
  - pippy tests fail ([comment](https://github.com/pytorch/pytorch/pull/126578#issuecomment-2155836555))
- [Revert "[dynamo] Support if cond on UnspecializedNNModuleVariable and add inline tests (#128158)"](https://github.com/pytorch/pytorch/commit/6e13c7e8745d80e14d77dc2c5cb1fd666959fbba)
  - pippy tests fail ([comment](https://github.com/pytorch/pytorch/pull/128158#issuecomment-2155835787))
- [Revert "[dynamo] Inline the getattr of fx graph and proxy graph (#128172)"](https://github.com/pytorch/pytorch/commit/94165dba7b96f5ac0de5c95560915b7bce3af21e)
  - pippy tests fail ([comment](https://github.com/pytorch/pytorch/pull/128172#issuecomment-2155835201))
- [Revert "Change lerp decomp to use aten.as_strided_copy instead of prims.copy_strided (#128030)"](https://github.com/pytorch/pytorch/commit/0ef522956943e3f0398b6d2bf9ee1ac0a5a3130d)
  - breaking cuda12.1 test_cuda, see HUD https://hud.pytorch.org/hud/pytorch/pytorch/main/1?per_page=50&name_filter=inductor ([comment](https://github.com/pytorch/pytorch/pull/128030#issuecomment-2155764546))
- [Revert "Added memory budget to partitioner (#126320)"](https://github.com/pytorch/pytorch/commit/128952625beb5bcce3601ecab79626d5fac914c3)
  - The new test_ac.py fails on ROCm machines ([comment](https://github.com/pytorch/pytorch/pull/126320#issuecomment-2155141886))
- [Revert "[DDP] Bucket handling: make first bucket size equal to bucket_cap_mb if it was set (#121640)"](https://github.com/pytorch/pytorch/commit/9795c4224bcfb317a517b966f00ac78a2debee22)
  - Sorry but it looks like you're failing  `distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_bucketing_coalesced_op `. THe build failed so the tests didn't run, consider rebasing, there have been a couple of PRs lately related to cudnn so you probably are either based on a bad or too old of a commit https://hud.pytorch.org/pytorch/pytorch/commit/e98662bed99df57b7d79f9fc1cbe670afc303235 https://github.com/pytorch/pytorch/actions/runs/9392731942/job/25868060913 ([comment](https://github.com/pytorch/pytorch/pull/121640#issuecomment-2151258585))
- [Revert "Complete revamp of float/promotion sympy handling (#126905)"](https://github.com/pytorch/pytorch/commit/d5cb5d623aefcb0928d80f226d1a4962706ada38)
  - internal user reported ceiling equality simplification problem, I have a plan ([comment](https://github.com/pytorch/pytorch/pull/126905#issuecomment-2148805840))
- [Revert "Inductor: Allow small sizes of m for mixed mm autotuning (#127663)"](https://github.com/pytorch/pytorch/commit/907cb28f676a6d3f44d6f3a2503c56888ebecc93)
  - breaks torch ao CI, see: https://github.com/pytorch/pytorch/issues/127924 ([comment](https://github.com/pytorch/pytorch/pull/127663#issuecomment-2148554128))
- [Revert "FP8 rowwise scaling (#125204)"](https://github.com/pytorch/pytorch/commit/d05cddfe2327a92a62fba2220d5d3f735e58d40d)
  - Broke nightlies and internal tests ([comment](https://github.com/pytorch/pytorch/pull/125204#issuecomment-2145422196))

### Weird (3)

- [Revert "Use hidden visibility in OBJECTCXX files (#127265)"](https://github.com/pytorch/pytorch/commit/75b0720a97ac5d82e8a7a1a6ae7c5f7a87d7183d)
  - Sorry for reverting your change, but I suspect that it causes this failure https://github.com/pytorch/vision/issues/8478 on vision where its C++ extension could not be loaded on macOS ([comment](https://github.com/pytorch/pytorch/pull/127265#issuecomment-2156401838))
- [Revert "[BE]: Update cudnn to 9.1.0.70 (#123475)"](https://github.com/pytorch/pytorch/commit/9a8ab778d34bd24c5caceb340837483decc4c311)
  - CUDA trunk jobs are pretty red after this change, and the forward fix https://github.com/pytorch/pytorch/pull/127984 does not look working ([comment](https://github.com/pytorch/pytorch/pull/123475#issuecomment-2149258430))
- [Revert "[inductor] Enable subprocess-based parallel compile as the default (#126817)"](https://github.com/pytorch/pytorch/commit/9a25ff77af932b59899f337a7a8dffbaab166ecf)
  - There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))
