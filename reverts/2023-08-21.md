# Week of 2023-08-21 to 2023-08-28 (22)

### GHFirst (14)

- [Revert "[quant][pt2e][xnnpack_quantizer] Add support for mul and mul_relu (#107930)"](https://github.com/pytorch/pytorch/commit/8d44b0f5a5e653096d39301ac34e07738a0705a0)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/107930#issuecomment-1694069330))
- [Revert "Remove remaining global `set_default_dtype` calls from tests (#107246)"](https://github.com/pytorch/pytorch/commit/161ea463e690dcb91a30faacbf7d100b98524b6b)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/107246#issuecomment-1693838522))
- [Revert "[Dynamo] cache_size policy (#107496)"](https://github.com/pytorch/pytorch/commit/b4c6c4da88f87b979c8c4e1a2059d4e1c2e1988b)
  - Breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/107496#issuecomment-1693590121))
- [Revert "[dynamo] Treat monkey patched .forward as dynamic (#107104)"](https://github.com/pytorch/pytorch/commit/eefce56b6617bea4744ebd1654d95753bcb5956a)
  - Breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/107104#issuecomment-1692072018))
- [Revert "inductor: remove conv_bn folding from pre_grad pass (#106686)"](https://github.com/pytorch/pytorch/commit/2fcda650cf29301e86dd21a5ceabc0f31fa4e3d3)
  - Has big accuracy drop for internal models test ([comment](https://github.com/pytorch/pytorch/pull/106686#issuecomment-1690972043))
- [Revert "enable conv+bn folding for mixed-dtype when bn has post activation (#107142)"](https://github.com/pytorch/pytorch/commit/3af04ce0ffe70e3e8f014f19e269022e35b20852)
  - [Depends on reverted https://github.com/pytorch/pytorch/pull/106576](https://github.com/pytorch/pytorch/pull/106686) ([comment](https://github.com/pytorch/pytorch/pull/107142#issuecomment-1690968509))
- [Revert "reseed all Generators in Dataloader's _worker_loop() -- via GC (#107131)"](https://github.com/pytorch/pytorch/commit/ecde622649d2c6fbca088d6e253ebadb344a12d4)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/107131#issuecomment-1690325745))
- [Revert "[ATen] Update pre-compiled header (#106915)"](https://github.com/pytorch/pytorch/commit/e573abec123a84ab293a7f04f27a10223dbb4b9b)
  - reverting the full stack. I missed that the iostream pr was stacked under this one and it's builds are also failing internally ([comment](https://github.com/pytorch/pytorch/pull/106915#issuecomment-1689087860))
- [Revert "[inductor] Adjust dynamic SMEM limit when above default in AOT (#107601)"](https://github.com/pytorch/pytorch/commit/42897e81270ddad2449504d4fe38a673f8a95a07)
  - Sorry, but the test added in this PR breaks when run internally. See D48549503 for more details ([comment](https://github.com/pytorch/pytorch/pull/107601#issuecomment-1689049609))
- [Revert "Remove CUTLASS extensions merged upstream (#107612)"](https://github.com/pytorch/pytorch/commit/8fb6416bfa403d25df1c32846e71b31b79fc88d5)
  - Sorry, this breaks internal builds which still depend on these files ([comment](https://github.com/pytorch/pytorch/pull/107612#issuecomment-1688936837))
- [Revert "Wrap indirect indexing on CUDA (#105055)"](https://github.com/pytorch/pytorch/commit/b28278740955ba15d8d33d7f39370edfd710fc5a)
  - Causes failure in inductor_torchbench ([comment](https://github.com/pytorch/pytorch/pull/105055#issuecomment-1688871947))
- [Revert "[BE]: Update ruff to 0.285 (#107519)"](https://github.com/pytorch/pytorch/commit/d59a6864fb2481cda28e937c8082ffa072f7b3dd)
  - Sorry, but this PR breaks internal tests. @ezyang, can you please hep them get unblocked? It seems like one of the strings was prob accidentally modified ([comment](https://github.com/pytorch/pytorch/pull/107519#issuecomment-1688833480))
- [Revert "Remove some unnecessary <iostream> includes from headers (#106914)"](https://github.com/pytorch/pytorch/commit/28dc1a093f49432ab15aafa1f662d7ed6c86cb05)
  - Sorry, but this is breaking internal builds. Seems like a lot of internal code depends on some of the removed imports ([comment](https://github.com/pytorch/pytorch/pull/106914#issuecomment-1688605975))
- [Revert "Add scalar conversion using avx instructions for half (#102140)"](https://github.com/pytorch/pytorch/commit/e0f1fe102ac85bcf3b7b01e50fa56df1bcbbfef1)
  - Sorry, this is still breaking internal builds. Specifically, the dynamo test test_repros.py::DynamicShapesReproTests::test_odict_get_item_index_name ([comment](https://github.com/pytorch/pytorch/pull/102140#issuecomment-1686684117))

### Ignored Signal (1)

- [Revert "pt2: make aot_eager backend handle basic float8 operations (#107642)"](https://github.com/pytorch/pytorch/commit/5025fb9213e000ff78c40a81b4e56641773d327b)
  - Sorry for reverting this, but it is failing Windows CPU test in trunk. The Windows failures on your PR looks related I think ([comment](https://github.com/pytorch/pytorch/pull/107642#issuecomment-1688999380))

### Landrace (2)

- [Revert "Handle 2D blocking with foreach (#107840)"](https://github.com/pytorch/pytorch/commit/d35d7de60eb681d1395ebb9a461d9fc0ac031155)
  - Sorry for reverting this, but test_2d_blocking is failing in trunk, probably a landrace as PR was green ([comment](https://github.com/pytorch/pytorch/pull/107840#issuecomment-1694009217))
- [Revert "Fakify leaf of FunctionalTensor (#107062)"](https://github.com/pytorch/pytorch/commit/96c5be8bc403c278acda0cc34bf01dca7b4434e4)
  - This appears to have broken the test TestDTensorCompile.test_dtensor_fullgraph. Probably a land race ([comment](https://github.com/pytorch/pytorch/pull/107062#issuecomment-1685447747))

### Not through pytorchbot (2)

- [Revert the removal of a SampleInput for gather (#107776)](https://github.com/pytorch/pytorch/commit/977aba7cfe84bf8fec60330a97a10445fabe3765)
- [Back out "[inductor] make thread order consistent with loop order (#106827)" (#107796)](https://github.com/pytorch/pytorch/commit/398f4ae4515dbf5855061d4bbf7a009df39c7bb3)

### Weird (3)

- [Revert "[optim] Make casting to match params a hook (#106725)"](https://github.com/pytorch/pytorch/commit/3a3cf0e09d475df9237c95ebd14debf650e0c038)
  - We acknowledge this is a huge risk because people do not remember to call super().__init__ from their Optimizer subclasses and so this will break lots of load_state_dict behavior ([comment](https://github.com/pytorch/pytorch/pull/106725#issuecomment-1693386137))
- [Revert "Add tensor post accumulate grad hook API (#107063)"](https://github.com/pytorch/pytorch/commit/432fce4e0d015de0438c28fb518bfa4b6fb6bb8f)
  - Diff train weirdness. Need to temporarily revert this PR and will right land it soon afterwards ([comment](https://github.com/pytorch/pytorch/pull/107063#issuecomment-1690799057))
- [Revert "Remove unnecessary import in python_variable.cpp (#107794)"](https://github.com/pytorch/pytorch/commit/bc0790559b8a6ac5bb5d15670abbc5d1b2dfdc4f)
  - Diff train weirdness. Need to temporarily revert this PR and will right land it soon afterwards ([comment](https://github.com/pytorch/pytorch/pull/107794#issuecomment-1690798855))
