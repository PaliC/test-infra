# Week of 2024-04-29 to 2024-05-06 (19)

### GHFirst (6)

- [Revert "Add `write_record_metadata` to PyTorchFileWriter (#125184)"](https://github.com/pytorch/pytorch/commit/ccbac091d20f51d19b3278a1ab845ac57c9042e9)
  - breaks internal builds, see D56962076 ([comment](https://github.com/pytorch/pytorch/pull/125184#issuecomment-2094976897))
- [Revert "Include support for the scatter gather cuda kernels to allow for comp… (#124809)"](https://github.com/pytorch/pytorch/commit/a0e2f62edd79ebdc21c3b8c16bf480db1af168a0)
  - breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/124809#issuecomment-2091751002))
- [Revert "[dynamo] use lazy disable dynamo for manual seed (#125196)"](https://github.com/pytorch/pytorch/commit/b03fb49ed8d812ceba3741432ac1f9de3bd1650e)
  - breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/125196#issuecomment-2089355842))
- [Revert "Fakify script object inputs and attributes for non-strict export (#124239)"](https://github.com/pytorch/pytorch/commit/f1f142c44f81384afbdba5e451fc15744868bf26)
  - breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/124239#issuecomment-2089305447))
- [Revert "Add registration API for torch.compile-eager (#121387)"](https://github.com/pytorch/pytorch/commit/ca0f070065e8bf4575107ccdecd6785dea6d9010)
  - breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/121387#issuecomment-2087541956))
- [Revert "Fix & optimze open device registration test. (#124712)"](https://github.com/pytorch/pytorch/commit/ea347fa6cef24cc0938b1d689f4e68c096877b81)
  - breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/124712#issuecomment-2086971499))

### Ignored Signal (1)

- [Revert "Include support for the scatter gather cuda kernels to allow for comp… (#124809)"](https://github.com/pytorch/pytorch/commit/4d410155b241334b32872ff593d3480d10f57d5e)
  - windows build failure is real, https://github.com/pytorch/pytorch/actions/runs/8910674030/job/24470387612#step:11:11236 is the correct failure line, ignore the statement saying build passed, batch is errorcodes arent propagating again ([comment](https://github.com/pytorch/pytorch/pull/124809#issuecomment-2088680371))

### Not through pytorchbot (1)

- [Revert "[benchmark][cudagraph] Explicitly call aten.div with CUDA denominator for cudagraphs (#119729)" (#125246)](https://github.com/pytorch/pytorch/commit/c12c85e9199c683fe047883323c61d82fdbbd262)

### No Signal (4)

- [Revert "[Inductor] Properly package target info for triton.compile  (#125241)"](https://github.com/pytorch/pytorch/commit/6d30803d64953955df63da56833bf4eb52249aae)
  - Sorry for reverting your change but it is failing inductor tests on ROCm https://hud.pytorch.org/pytorch/pytorch/commit/8a1af95b0979d85c4fe32a75e797323ad81f298d ([comment](https://github.com/pytorch/pytorch/pull/125241#issuecomment-2094472886))
- [Revert "Convert `ForeachFuncInfo` to `dataclass` (#125001)"](https://github.com/pytorch/pytorch/commit/75fa54a9d1117e67f3ac2ebb3f4187fd47101acf)
  - Sorry for reverting your change but I think it is breaking on ROCm https://hud.pytorch.org/pytorch/pytorch/commit/9466335ae4cb049efd3f4c2b32b2115ba00694f3 ([comment](https://github.com/pytorch/pytorch/pull/125001#issuecomment-2086640674))
- [Revert " [Distributed] [7/N] Fix clang-tidy warnings in torch/csrc/distributed/c10d  (#124987)"](https://github.com/pytorch/pytorch/commit/724c7491d063f5d564f2b94ecde02b1f2db7ddd6)
  - broke downstream extensions ([comment](https://github.com/pytorch/pytorch/pull/124987#issuecomment-2083956511))
- [Revert "[Meta Tensor] fix meta inplace set storage (#123880)"](https://github.com/pytorch/pytorch/commit/ae13c7e593c1032fa3f96f4aa1b3379547a3a9f3)
  - breaks cpu_inductor_torchbench (detectron2_fasterrcnn) ([comment](https://github.com/pytorch/pytorch/pull/123880#issuecomment-2083366385))

### Weird (7)

- [Revert "[FSDP2] Computed grad divide factors at runtime (#125484)"](https://github.com/pytorch/pytorch/commit/dba689bbfdc65cf12711ec4b4f2f7eed0fae3a59)
  - Sorry for reverting your change, I am trying to restore ROCm distributed failures in trunk https://hud.pytorch.org/pytorch/pytorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb ([comment](https://github.com/pytorch/pytorch/pull/125484#issuecomment-2094646996))
- [Revert "try to fix the warning in distribute_tensor (#125476)"](https://github.com/pytorch/pytorch/commit/084d818e7120398f5c9a55a7330265d6e1f3bdb4)
  - Sorry for reverting your change, but there are real failures on the PR that sneak in during the log classifier outage ([comment](https://github.com/pytorch/pytorch/pull/125476#issuecomment-2094468740))
- [Revert "Don't call item() into torch.scalar_tensor uselessly (#125373)"](https://github.com/pytorch/pytorch/commit/a32ad828dcdff7acfe79b7c0cc260486b783f6d7)
  - Sorry for reverting your change, but there are real failures on the PR that sneak in during the log classifier outage ([comment](https://github.com/pytorch/pytorch/pull/125373#issuecomment-2094464241))
- [Revert "Refactoring to remove unused variable (#125252)"](https://github.com/pytorch/pytorch/commit/a9309502afa95e6c4d1cc87c150c2ca9e0c41c37)
  - going to land codev ([comment](https://github.com/pytorch/pytorch/pull/125252#issuecomment-2089394606))
- [Revert "CI: add aarch64 linux workflow (#121284)"](https://github.com/pytorch/pytorch/commit/e7631d6eae30ad69bd21ab4ac01fdc4268a0469e)
  - Test only changes has not been reverted ([comment](https://github.com/pytorch/pytorch/pull/121284#issuecomment-2083925890))
- [Revert "[dtensor] implement shard dim change with alltoall (#124872)"](https://github.com/pytorch/pytorch/commit/f1d1e3246f3203a4c9641fcda28b0ed66eb8f4d4)
  - broke distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_True https://github.com/pytorch/pytorch/actions/runs/8882762411/job/24389191482 https://hud.pytorch.org/pytorch/pytorch/commit/f7f018a0ed442f92eb5270150ced7b6117773368.  Bad TD ([comment](https://github.com/pytorch/pytorch/pull/124872#issuecomment-2083599445))
- [Revert "[dtensor] delete the old unused mesh_alltoall (#124879)"](https://github.com/pytorch/pytorch/commit/3bd67dab324442a28243439461d375c74a466f27)
  - broke distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_True https://github.com/pytorch/pytorch/actions/runs/8882762411/job/24389191482 https://hud.pytorch.org/pytorch/pytorch/commit/f7f018a0ed442f92eb5270150ced7b6117773368.  Bad TD ([comment](https://github.com/pytorch/pytorch/pull/124872#issuecomment-2083599445))
