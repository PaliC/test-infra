# Week of 2023-10-16 to 2023-10-23 (20)

### GHFirst (2)

- [Revert " [1/N] Apply clang-tidy to c10 cuda files (#111137)"](https://github.com/pytorch/pytorch/commit/9c7391ea369951f419ed6fa74e0de146185bb301)
  - Was reverted internally due to the failures in torch.cuda.memory_stats(device=0) (presumably) ([comment](https://github.com/pytorch/pytorch/pull/111137#issuecomment-1769274103))
- [Revert "Reland #2 "[C10] PG observability hooks. (#108815, #110907)" (#111072)"](https://github.com/pytorch/pytorch/commit/1e70f4d02cbd2c5736ae3b80558b00ee0e953eac)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/111072#issuecomment-1765399829))

### Ignored Signal (1)

- [Revert "[ROCm] Unskip functorch tests that now work (#110760)"](https://github.com/pytorch/pytorch/commit/c2a248bdb3a1b70e30e526505f1504803de7737b)
  - Lint failure ([comment](https://github.com/pytorch/pytorch/pull/110760#issuecomment-1773490896))

### Landrace (1)

- [Revert "update int4 tinygemm kernels (#111327)"](https://github.com/pytorch/pytorch/commit/5ff9b49063593687f2c26de90c3069a21e601ab5)
  - This PR is preventing the revert of https://github.com/pytorch/pytorch/pull/110914 ([comment](https://github.com/pytorch/pytorch/pull/111327#issuecomment-1765299310))

### Not through pytorchbot (1)

- [Back out "Add a workflow to release Android binaries (#110976)" (#111401)](https://github.com/pytorch/pytorch/commit/bd9a2465e72ee4c6fc838a38c86d93a92da29eaa)

### No Signal (15)

- [Revert "Use fmt::format in NCCLUtils and ProcessGroupNCCL instead of c10::str (#107268)"](https://github.com/pytorch/pytorch/commit/f0cde8613c4c8814e157c0a742187a91aa72a009)
  - Breaks build on Ubuntu 23.04 ([comment](https://github.com/pytorch/pytorch/pull/107268#issuecomment-1773960355))
- [Revert "Nvfuser code removal (#111093)"](https://github.com/pytorch/pytorch/commit/715dfced72657e5adacd5bef16e3d458cd94851b)
  - Breaking internal builds, @albanD please help to support the author with the next steps to get this diff merged ([comment](https://github.com/pytorch/pytorch/pull/111093#issuecomment-1771434853))
- [Revert "[Compiled Autograd] Turn accumulate_grad into an op (#111271)"](https://github.com/pytorch/pytorch/commit/3eb5cae3af1207ac58f77c5ac78669e276824cb9)
  - Breaking internal CI ([comment](https://github.com/pytorch/pytorch/pull/111271#issuecomment-1768527932))
- [Revert "[Compiled Autograd] Error if tensor_post_acc_grad_hooks is set (#111273)"](https://github.com/pytorch/pytorch/commit/0be90c5d7ffe6b82026544556a3b4543f09fa092)
  - Breaking internal CI ([comment](https://github.com/pytorch/pytorch/pull/111273#issuecomment-1768522328))
- [Revert "[inductor] Move inductor ops to CompositeExplicitAutograd (#111274)"](https://github.com/pytorch/pytorch/commit/a389e2c7c7881a5fd29d3097671429c299eedda5)
  - Breaking internal CI ([comment](https://github.com/pytorch/pytorch/pull/111274#issuecomment-1768517555))
- [Revert "[aot_inductor] return a copy of any constant (#111356)"](https://github.com/pytorch/pytorch/commit/ed7739d690af78f0cedcfdf2139801dcd2d7078e)
  - Breaking internal ci ([comment](https://github.com/pytorch/pytorch/pull/111356#issuecomment-1768503640))
- [Revert "[inductor] Refactor and optimize allocation calls (#111117)"](https://github.com/pytorch/pytorch/commit/08f580d49822abd46d004e6965f74861fe7a750c)
  - Braking internal CI ([comment](https://github.com/pytorch/pytorch/pull/111117#issuecomment-1768489865))
- [Revert "[sparse] semi-structured sparse + torch.compile support (#111049)"](https://github.com/pytorch/pytorch/commit/41490119f252e6192db3c29b674dbba4782a12e6)
  - Sorry I'm pretty sure this caused a memory leak https://hud.pytorch.org/pytorch/pytorch/commit/408f210938176870133a3dde5e8fbc4926cafbc0 https://github.com/pytorch/pytorch/actions/runs/6550388354/job/17790615103 `test_sparse_semi_structured.py::TestSparseSemiStructuredCUDA::test_mlp_contiguous_relu_compile_backend_cutlass_dense_input_shape_(1, 128)_cuda - RuntimeError: CUDA driver API confirmed a leak in __main__.TestSparseSemiStructuredCUDA.test_mlp_contiguous_relu_compile_backend_cutlass_dense_input_shape_(1, 128)_cuda! Caching allocator allocated memory was 235008 and is now reported as 352256 on device 0. CUDA driver allocated memory was 359333888 and is now 361431040.` ([comment](https://github.com/pytorch/pytorch/pull/111049#issuecomment-1767186569))
- [Revert "direct runtime assertions (#111262)"](https://github.com/pytorch/pytorch/commit/7a740e2b85c358bb6a2aceab92ff213c9be1908d)
  - Breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/111262#issuecomment-1765881675))
- [Revert "Quant: add weight int4pack mm kernel (#110914)"](https://github.com/pytorch/pytorch/commit/408e991dfe030176dc7cc9439f99c184a5f97238)
  - Breaking internal builds ([comment](https://github.com/pytorch/pytorch/pull/110914#issuecomment-1765302621))
- [Revert "Add `lazy_clone_storage` to create COW storages (#110192)"](https://github.com/pytorch/pytorch/commit/97a513ed077323550b808e690a0b5a0452f87334)
  - Breaking internal builds, @ezyang please support the author providing further details ([comment](https://github.com/pytorch/pytorch/pull/110192#issuecomment-1765157285))
- [Revert "[inductor] Adding a way to force fusion of int_mm with mul (#111125)"](https://github.com/pytorch/pytorch/commit/89f11c69a8eb53fef7d28581abf0e9ee626aac61)
  - Sorry for reverting your change, but it fails on ROCm https://hud.pytorch.org/pytorch/pytorch/commit/f4297576e63e4110f6bdf2522ae6a5fb4c7f3816 ([comment](https://github.com/pytorch/pytorch/pull/111125#issuecomment-1764956174))
- [Revert "[C10D] Introduce C++ side Collective Callbacks. (#110307)"](https://github.com/pytorch/pytorch/commit/493618d7451063d0fd2abd24cd06333791ad2381)
  - this sits on top of another PR https://github.com/pytorch/pytorch/pull/111072 that needs to be reverted due to internal release testing failure / multisect blame ([comment](https://github.com/pytorch/pytorch/pull/110307#issuecomment-1764910301))
- [Revert "[vision hash update] update the pinned vision hash (#111316)"](https://github.com/pytorch/pytorch/commit/9af82fa2b86fb71df503082b1960c9392f9dc66d)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/111316#issuecomment-1763827734))
- [Revert "[sparse] semi-structured sparse + torch.compile support (#111049)"](https://github.com/pytorch/pytorch/commit/b4745d476ceee1c01646818ce398b3d3a74b592d)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/111049#issuecomment-1763795957))
