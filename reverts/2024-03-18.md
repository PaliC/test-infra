# Week of 2024-03-18 to 2024-03-25 (35)

### GHFirst (5)

- [Revert "Proper view support for jagged layout NestedTensor (#113279)"](https://github.com/pytorch/pytorch/commit/224beecee6f4b4b8be8026e90c5f852000c144d6)
  - Need to fix BC thing ([comment](https://github.com/pytorch/pytorch/pull/113279#issuecomment-2013899762))
- [Revert "Support for torch.nested.as_nested_tensor(t) (#113280)"](https://github.com/pytorch/pytorch/commit/12e7602cf9bf69423e7590f1d5bac5aeeece71af)
  - Need to fix BC thing ([comment](https://github.com/pytorch/pytorch/pull/113280#issuecomment-2013893099))
- [Revert "Public API for NJT construction from jagged components (#121518)"](https://github.com/pytorch/pytorch/commit/816db3bd292bd24b6e21d8fd34f957118fa3f254)
  - Need to fix BC thing ([comment](https://github.com/pytorch/pytorch/pull/121518#issuecomment-2013879641))
- [Revert "Enable x86 CPU vectorization on windows [submodule sleef] (#118980)"](https://github.com/pytorch/pytorch/commit/25bf5f7e61b98a7d9e04645b4eb322cc0541b389)
  - Sorry for revert your change one more time but the hard part is that it breaks lot of internal builds ([comment](https://github.com/pytorch/pytorch/pull/118980#issuecomment-2013043364))
- [Revert "[aot_inductor][easy] enable test_triton_kernel_multi_output_arg (#122052)"](https://github.com/pytorch/pytorch/commit/c71554b9449749350dd0273e8508d8168a87bdd6)
  - Although this look fixed on OSS, it is still failing internally.  I have added the reproducible buck command in the diff D55046262 ([comment](https://github.com/pytorch/pytorch/pull/122052#issuecomment-2008253185))

### Ignored Signal (2)

- [Revert "Change ATEN generator argument type to const std::optional<Generator>& (#120076)"](https://github.com/pytorch/pytorch/commit/02fee6caec99c36b1ca55f98211f7728f1ea40d7)
  - Reverting in order to check if this will fix XLA trunk jobs ([comment](https://github.com/pytorch/pytorch/pull/120076#issuecomment-2015272644))
- [Revert "[dynamo] Forward OptimizedModule.__setattr__ to the wrapped module (#122098)"](https://github.com/pytorch/pytorch/commit/e5e0685f614171ab3e3a22c85dd4ee8cd0cf09ae)
  - Sorry for reverting your change but the distributed failure looks legit as it is also failing in trunk https://hud.pytorch.org/pytorch/pytorch/commit/88ebdbc97c103271766203df6662240e95a09b42 ([comment](https://github.com/pytorch/pytorch/pull/122098#issuecomment-2008483316))

### Landrace (1)

- [Revert "Factor meta conversion through serializable MetaTensorDesc (#122044)"](https://github.com/pytorch/pytorch/commit/f65373e27803647b2b509142510e434630b3f661)
  - Seems that some landrace caused this PR to break lint ([comment](https://github.com/pytorch/pytorch/pull/122044#issuecomment-2015025490))

### Not through pytorchbot (2)

- [Back out "[fx] Preserve Fx graph node order in partitioner across runs (#115621)" (#122113)](https://github.com/pytorch/pytorch/commit/8de4d864796e16a00945c983e4683675f117521c)
- [Back out "Support `triton.language.dtype` with `torch.compile` (#121690)" (#122108)](https://github.com/pytorch/pytorch/commit/7c5e29ae71268882a3c03db7a9511e25e1d8c4ae)

### No Signal (13)

- [Revert "[Inductor] Make codecache CUDA compilation more robust & flexible (#121490)"](https://github.com/pytorch/pytorch/commit/3795ebe9250cd23e5990884c9155bd233dac26ed)
  - Sorry for reverting you change but I think it is failing on ROCm, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/700c92e1b9cb6fae2610d08e5a960273c4dd1697 ([comment](https://github.com/pytorch/pytorch/pull/121490#issuecomment-2015829464))
- [Revert "[Inductor Cutlass backend] GEMM size threshold for Cutlass backend usage (#121491)"](https://github.com/pytorch/pytorch/commit/97d3bf71b9b7e32aec44cc44677a75f4b2a1197d)
  - Sorry for reverting you change but I think it is failing on ROCm, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/700c92e1b9cb6fae2610d08e5a960273c4dd1697 ([comment](https://github.com/pytorch/pytorch/pull/121490#issuecomment-2015829464))
- [Revert "[Quant] [PT2] Add SiLU  into X86InductorQuantizer Conv2d Unary Annotation (#122267)"](https://github.com/pytorch/pytorch/commit/60bc29aa0b22dcc06c5778589a9d2b5c4b9fc965)
  - Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it ([comment](https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491))
- [Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op SiLU (#122268)"](https://github.com/pytorch/pytorch/commit/b30b396d057f3462fbf2a1b46b8f480947dcb6d0)
  - Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it ([comment](https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491))
- [Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardSwish with int8-mix-bf16 (#122373)"](https://github.com/pytorch/pytorch/commit/777ac511cc98a7edf916e61d0770afac4e37fccc)
  - Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it ([comment](https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491))
- [Revert "[Quant] [Inductor] Enable the Inductor Lowering of QConv2d post op HardTanh with int8-mix-bf16 (#122374)"](https://github.com/pytorch/pytorch/commit/dbedc6bb7cf4cfaab5df85aff2e4dbaf049e6746)
  - Not sure if this PR caused breakages in main rocm jobs, I'll remerge if reverting does not fix it ([comment](https://github.com/pytorch/pytorch/pull/122267#issuecomment-2015294491))
- [Revert "Optimize multi_tensor_apply (take 2) (#119764)"](https://github.com/pytorch/pytorch/commit/5e0440edb467b79c3779454eeb5d5d24bbec9c17)
  - Sorry for reverting your change but it is failing ROCm job in trunk https://hud.pytorch.org/pytorch/pytorch/commit/0b68a28c87df2c6eb2cf530be4659b5a2f8a95b0.  Please help take a look and reland the change ([comment](https://github.com/pytorch/pytorch/pull/119764#issuecomment-2014190124))
- [Revert "Precompile triton templates (#121998)"](https://github.com/pytorch/pytorch/commit/bce640709c37389ff1d3c31f44967a9745973e57)
  - Sorry for reverting your change but it is causing all ROCm trunk job to fail https://hud.pytorch.org/pytorch/pytorch/commit/b8df2f0ca530ebe01fa079c891c170a1f4b22823 ([comment](https://github.com/pytorch/pytorch/pull/121998#issuecomment-2014003037))
- [Revert "Refactor gpu trace to be device-agnostic (#121794)"](https://github.com/pytorch/pytorch/commit/968c4c4154caaf570348349fbc29d7516a5984bb)
  - Sorry for reverting your change but it breaks ROCm jobs in trunk https://hud.pytorch.org/pytorch/pytorch/commit/74deacbf31d032a2659dc1633dc3e5248921d466, please help take a look and reland the change ([comment](https://github.com/pytorch/pytorch/pull/121794#issuecomment-2013674083))
- [Revert "Support gpu trace on XPU (#121795)"](https://github.com/pytorch/pytorch/commit/13afbcfc85d5c613b08111c6b2aa84b292b6f20d)
  - Sorry for reverting your change but it breaks ROCm jobs in trunk https://hud.pytorch.org/pytorch/pytorch/commit/74deacbf31d032a2659dc1633dc3e5248921d466, please help take a look and reland the change ([comment](https://github.com/pytorch/pytorch/pull/121794#issuecomment-2013674083))
- [Revert "[Sparsity] add support for H100 compute capability 9.x (#121768)"](https://github.com/pytorch/pytorch/commit/0d8e960f74acd359358e0b729c4803d2b71849e5)
  - Agreed on reverting and fixing rocm tests ([comment](https://github.com/pytorch/pytorch/pull/121768#issuecomment-2011893826))
- [Revert "Teach dynamo about torch.func.jvp (#119926)"](https://github.com/pytorch/pytorch/commit/0696db820266c1402315f78458d57ea1cd48a2a7)
  - broken mac jobs on main ([comment](https://github.com/pytorch/pytorch/pull/119926#issuecomment-2010327997))
- [Revert "Update jvp to support symbolic execution. (#120338)"](https://github.com/pytorch/pytorch/commit/65eb22158e9d031ae0d6e31ac771ca4847236b3d)
  - Broke dynamo tests on trunk ([comment](https://github.com/pytorch/pytorch/pull/120338#issuecomment-2010276712))

### Weird (12)

- [Revert "Introduce XPU implementation for PyTorch ATen operators (#120891)"](https://github.com/pytorch/pytorch/commit/182bb0f2ca7758d174119476eca7f6a548ea3980)
  - Sorry for reverting your change but I need to revert it to resolve a conflict in trunk https://github.com/pytorch/pytorch/pull/121794#issuecomment-2013434523.  Please help reland the change after ([comment](https://github.com/pytorch/pytorch/pull/120891#issuecomment-2013668563))
- [Revert "Avoid COW materialize in conv, log sigmoid, repeat, group_norm, batch_norm (#121537)"](https://github.com/pytorch/pytorch/commit/c80601f35ae070bfeea5e1d860fb7e29bf8348dd)
  - flaky CI failures ([comment](https://github.com/pytorch/pytorch/pull/121537#issuecomment-2010937226))
- [Revert "Enable x86 CPU vectorization on windows [submodule sleef] (#118980)"](https://github.com/pytorch/pytorch/commit/15a8185cd332f691b2cd1b58971ebca5034add31)
  - This caused build failures for 2+ pytorch devs, so we're reverting it to be safe ([comment](https://github.com/pytorch/pytorch/pull/118980#issuecomment-2009661069))
- [Revert "Upgrade submodule sleef to fix build warning (#122168)"](https://github.com/pytorch/pytorch/commit/06db0a9f78fa641a687f83fd2093941ac41fb1d3)
  - trying to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/122168#issuecomment-2009653474))
- [Revert "[torch export][serialize] create a more compact stacktrace format for serialization (#121675)"](https://github.com/pytorch/pytorch/commit/d56ab7b0204c3412958958a8f02bc244f2ab817c)
  - It seems that this PR broke lint jobs, I am reverting to confirm if this is the case ([comment](https://github.com/pytorch/pytorch/pull/121675#issuecomment-2007919486))
- [Revert "Teach dynamo about torch.func.jvp (#119926)"](https://github.com/pytorch/pytorch/commit/36e5c1dcabf1da86a472dd69b289dafe0c2c6211)
  - lots of breakages in pull jobs, checking if reverting this one will help ([comment](https://github.com/pytorch/pytorch/pull/119926#issuecomment-2007915919))
- [Revert "Update jvp to support symbolic execution. (#120338)"](https://github.com/pytorch/pytorch/commit/88999674a01413beb7d22f45a76d7c5afb697090)
  - lots of breakages in pull jobs, checking if reverting this one will help ([comment](https://github.com/pytorch/pytorch/pull/120338#issuecomment-2007898831))
- [Revert "Add Flash Attention support on ROCM (#121561)"](https://github.com/pytorch/pytorch/commit/764eae9c4e9b3187cda61482642703a2828923f9)
  - Sorry for reverting your change but this needs more work to be able to land in fbcode because https://github.com/ROCm/aotriton is not available there atm.  We are working to reland this change before 2.3 release ([comment](https://github.com/pytorch/pytorch/pull/121561#issuecomment-2007717091))
- [Revert "Refactor gpu trace to be device-agnostic (#121794)"](https://github.com/pytorch/pytorch/commit/f9ed1c432d56d1e9a59aea3c3d0dc988bfdec4a8)
  - Reverting to see if rocm trunk errors are related ([comment](https://github.com/pytorch/pytorch/pull/121794#issuecomment-2007519408))
- [Revert "Skip nonzero unbacked SymInt memo in inference mode (#122147)"](https://github.com/pytorch/pytorch/commit/7673cb534ade295ffd0b8ebfa9f0528b82945f7d)
  - Reverting to see if trunk error in inductor are related ([comment](https://github.com/pytorch/pytorch/pull/122147#issuecomment-2007513000))
- [Revert "[BE] Migrate pull.yml to use S3 pytorch-ci-artifacts bucket for linux-jammy-py3_8-gcc11 and docs builds/tests (#121908)"](https://github.com/pytorch/pytorch/commit/676a77177e552c8d1a4704bc98bfa0d5506c5313)
  - this is due to OIDC can't work on forked PR due to token write permissions can't be shared ([comment](https://github.com/pytorch/pytorch/pull/121908#issuecomment-2004707582))
- [Revert "[BE] Enables support for pytorch ci build in ARC + introduces _linux-build-rg.yml. (#121930)"](https://github.com/pytorch/pytorch/commit/ceb1910bada6dadd89eb910a708dd816284fa9a7)
  - New action is breaking current ci in not rebased PRs ([comment](https://github.com/pytorch/pytorch/pull/121930#issuecomment-2004393980))
