# Week of 2023-06-19 to 2023-06-26 (15)

### GHFirst (5)

- [Revert "Preserve original co_filename when FX symbolic_trace (#103885)"](https://github.com/pytorch/pytorch/commit/29e3fddb082b5a14262a7246bc62381a55199d45)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/103885#issuecomment-1603612781))
- [Revert "Use missing-prototypes in torch_cpu (#103725)"](https://github.com/pytorch/pytorch/commit/b5594f7df0e5eef241b9bf15f8c8b8bf777513b0)
  - Broke caffe2 builds due. More info at [D46920675](https://www.internalfb.com/diff/D46920675) ([comment](https://github.com/pytorch/pytorch/pull/103725#issuecomment-1603129273))
- [Revert "add override to Caffe2 (#103795)"](https://github.com/pytorch/pytorch/commit/626d8548df6f0c75f47c2233fd2ccd69e2ddad11)
  - Caused some breakages due to jobs using `-Winconsistent-missing-destructor-override` detecting inconsistent usage of override. Specifically the Tensor class destructor not being marked with override ([comment](https://github.com/pytorch/pytorch/pull/103795#issuecomment-1601812803))
- [Revert "[Reland][ET] Select used et_kernel_metadata only (#103705)"](https://github.com/pytorch/pytorch/commit/08a7d60a463ae505d519313b936c7597daa99657)
  - large number of internal failures in executorch contbuild. See [D46882119](https://www.internalfb.com/diff/D46882119) for more details ([comment](https://github.com/pytorch/pytorch/pull/103705#issuecomment-1601789900))
- [Revert "Ensure ncclCommAbort can abort stuck ncclCommInitRank (#103264)"](https://github.com/pytorch/pytorch/commit/f7737bb96badfce7c659e07e43ee1cf6891378a3)
  - This commits seems to have been causing failures in test_nccl_init_abort. Those failures may have been masked by pre-existing failures in the distributed jobs on trunk when running CI on this PR. Since those breaking changes are now reverted, we should be able to rebase this and get clean signal + uncover the breakages caused by this PR. ([comment](https://github.com/pytorch/pytorch/pull/103264#issuecomment-1599451197))

### Ignored Signal (1)

- [Revert "To add brief intro for CPU backend optimization (#103666)"](https://github.com/pytorch/pytorch/commit/e031dd23b0a6920a911a17cc59f4fd526d8a6a78)
  - Failing doc tests in trunk https://hud.pytorch.org/pytorch/pytorch/commit/013ffe457e79180d6aa3b82f20116052faee242a ([comment](https://github.com/pytorch/pytorch/pull/103666#issuecomment-1599301270))

### Landrace (3)

- [Revert "Migrate exportdb to torch.export from torchdynamo.export (#103861)"](https://github.com/pytorch/pytorch/commit/518abe8b7e5a5bb9c6a6759891bd6f1a56a8b4dc)
  - It looks like this change is failing in trunk due to a landrace https://hud.pytorch.org/pytorch/pytorch/commit/fb6173a4ac60ed5a22cff2c68134633eb72e53b9 ([comment](https://github.com/pytorch/pytorch/pull/103861#issuecomment-1601960600))
- [Revert "[pt2] grad support (#102264)"](https://github.com/pytorch/pytorch/commit/e737a8486f2b38c699d6890be295dd6e6d6417ea)
  - This is failing in trunk https://hud.pytorch.org/pytorch/pytorch/commit/85b83954c8820fc7473d8e7b68325fa8ed5753dc and looks like a landrace ([comment](https://github.com/pytorch/pytorch/pull/102264#issuecomment-1600001309))
- [Revert "[decomp] Decompose logaddexp2 (#103765)"](https://github.com/pytorch/pytorch/commit/7b6dc72ffae43d340a08ab79da8334a09bf41f14)
  - looks like land race ([comment](https://github.com/pytorch/pytorch/pull/103765#issuecomment-1599030496))

### Not through pytorchbot (2)

- [Back out "Revert "[DDP] multiple forward support for static graph (#103487)" (#103873)" (#103938)](https://github.com/pytorch/pytorch/commit/f044613f78df713fb57f70c608483c9f10ad332e)
- [Revert "[DDP] multiple forward support for static graph (#103487)" (#103873)](https://github.com/pytorch/pytorch/commit/b1ddd5a2936ec796b0bb837709d83357083ff63a)

### No Signal (4)

- [Revert "sparse_mask: backward support for sparse lhs (#95165)"](https://github.com/pytorch/pytorch/commit/727458239027222a630c72354adf42c28041ff6f)
  - Sorry for reverting this. I think one of the tests test_sparse.py::TestSparseCUDA::test_sparse_mask_backward_cuda_complex128 is failing on slow gradcheck https://hud.pytorch.org/pytorch/pytorch/commit/f090fdf3b49164679fb6316e9ae15e0c4fb3c9eb ([comment](https://github.com/pytorch/pytorch/pull/95165#issuecomment-1604696109))
- [Revert "_cycleviz.py: visualize reference cycles holding cuda memory (#102656)"](https://github.com/pytorch/pytorch/commit/7b3b6dd4262337c5289d64dd3e824b0614cf68e3)
  - Sorry for reverting your PR. But I think the change is failing on Windows CUDA https://github.com/pytorch/pytorch/actions/runs/5341701630/jobs/9683293600 ([comment](https://github.com/pytorch/pytorch/pull/102656#issuecomment-1603035364))
- [Revert "Reenable disabled tests by pr body (#103790)"](https://github.com/pytorch/pytorch/commit/58d11159bd9bdb4db70901821bcae4ce13160284)
  - I think we tested it on PR but missed the logic in trunk where there is no PR number ([comment](https://github.com/pytorch/pytorch/pull/103790#issuecomment-1601890299))
- [Revert "add oncall info individual info to failing alert job alert (#103915)"](https://github.com/pytorch/pytorch/commit/13664bb5356184f88aec7e4affc3c41cc377945f)
  - Broke trunk with no module named tools, see https://github.com/pytorch/pytorch/actions/runs/5338343319/jobs/9675586967 ([comment](https://github.com/pytorch/pytorch/pull/103915#issuecomment-1601802715))
