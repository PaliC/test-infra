# Week of 2024-04-08 to 2024-04-15 (19)

### GHFirst (10)

- [Revert "Simplify ATen sparse semi-structured operators based on CUTLASS (#123473)"](https://github.com/pytorch/pytorch/commit/97261be0a8f09bed9ab95d0cee82e75eebd249c3)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/123473#issuecomment-2053561077))
- [Revert "[sparse] Add fast semi-structured spasification kernels (#122350)"](https://github.com/pytorch/pytorch/commit/3120dbbf81f394bf7ecd0ea19da8729b2fcece65)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122350#issuecomment-2051757450))
- [Revert "Switch quantized_decomposed over to new custom ops API (#123454)"](https://github.com/pytorch/pytorch/commit/f0eb162730e76132a5e29adbc16f8721ef125d68)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/123454#issuecomment-2051738976))
- [Revert "[inductor] Fix fresh_inductor_cache() (#122661)"](https://github.com/pytorch/pytorch/commit/d994d993c05fdd93510dbaba4dcbfad4e4f20a1b)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122661#issuecomment-2051171028))
- [Revert "[inductor] Write generated files from parent process (#123409)"](https://github.com/pytorch/pytorch/commit/e881d567f402f6bf16c68803a1bf4bf5c5e1673f)
  - Needs to be reverted because it blocks reverting of the broken PR. ([comment](https://github.com/pytorch/pytorch/pull/123409#issuecomment-2051166617))
- [Revert "Add Matmul recipe into x86_inductor_quantizer (#122776)"](https://github.com/pytorch/pytorch/commit/5669334175bb2155316e7a74685b6278e127ecb4)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122776#issuecomment-2051073373))
- [Revert "[Quant][PT2E] Enable linear-binary(-unary) post-op recipe for X86Inductor quantizer (#122387)"](https://github.com/pytorch/pytorch/commit/8d9af8b91c275b06af45daf95ca3068c0a564fd5)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122387#issuecomment-2048294643))
- [Revert "[AOTI] Serialize large weights (#123002)"](https://github.com/pytorch/pytorch/commit/a65e9a06f057fb8352cb47ccb33d3879b2b46214)
  - There is conflict to land the diff internally ([comment](https://github.com/pytorch/pytorch/pull/123002#issuecomment-2048215990))
- [Revert "fix amp for AOTInductor (#122883)"](https://github.com/pytorch/pytorch/commit/b3eb1b2f74d16980dc832b73162245185f99a9ae)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122883#issuecomment-2046026363))
- [Revert "[EZ] Update mypy to 1.9.0 (#123595)"](https://github.com/pytorch/pytorch/commit/10d06fc92e322f33c5753a9f11e8322b3e320231)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/123595#issuecomment-2045865407))

### Ignored Signal (3)

- [Revert "Fix derived dim bugs in ep.run_decomp (#123326)"](https://github.com/pytorch/pytorch/commit/cf8139b956a44e52b4c44273bf6be90879d078c4)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/123326#issuecomment-2048389042))
- [Revert "Ignore logging.Logger.* calls during dynamo export (#123402)"](https://github.com/pytorch/pytorch/commit/d04957c0c682d766987cad07dce20986ca4a5b78)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/123402#issuecomment-2044236088))
- [Revert "Add test for skipping hf logging during export (#123410)"](https://github.com/pytorch/pytorch/commit/b9d2b75bac9ebc84eb7c3daa68d45cc0af1efda8)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/123402#issuecomment-2044236088))

### Landrace (1)

- [Revert "[quant] Enable backward for choose_qparams_per_token_asymmetric (#123452)"](https://github.com/pytorch/pytorch/commit/fe092da874d919f53d00f16a64a362469ce65218)
  - broke test_quantization.py::TestQuantizedTensor::test_decomposed_choose_qparams_per_token_asymmetric_backward on multiple jobs https://hud.pytorch.org/pytorch/pytorch/commit/c83900887f2fb5c7a04e7fd78ad8de7a20f356d4 https://github.com/pytorch/pytorch/actions/runs/8648781225/job/23714753103, probably a landrace ([comment](https://github.com/pytorch/pytorch/pull/123452#issuecomment-2050056601))

### No Signal (4)

- [Revert "Delete Lark (#123689)"](https://github.com/pytorch/pytorch/commit/6b18daf2056d94846aeb49b660126b41c0b7573a)
  - This PR seems to be breaking  test_binary_ufuncs.py ([comment](https://github.com/pytorch/pytorch/pull/123689#issuecomment-2048489549))
- [Revert "UFMT  formatting on test/export (#123520)"](https://github.com/pytorch/pytorch/commit/786c6db5194e740a231e7152eec694e40348204c)
  - lint is still broken ([comment](https://github.com/pytorch/pytorch/pull/123520#issuecomment-2046223260))
- [Revert "Swap to ID guard for optimizer Variable (#123496)"](https://github.com/pytorch/pytorch/commit/3e8d3577be0d80a32f90a1b06945b2c37860357f)
  - seems to have broken distributed/fsdp/test_fsdp_hybrid_shard.py as per https://hud.pytorch.org/pytorch/pytorch/commit/26bf05ccacc0377f0ef40d1d9c792c403267d5d5 ([comment](https://github.com/pytorch/pytorch/pull/123496#issuecomment-2043251234))
- [Revert "Defer setting capturable in optimizer variable (#123497)"](https://github.com/pytorch/pytorch/commit/d9ac80f80ce23dd45839fdbe1217c1963ffd7e9c)
  - seems to have broken distributed/fsdp/test_fsdp_hybrid_shard.py as per https://hud.pytorch.org/pytorch/pytorch/commit/26bf05ccacc0377f0ef40d1d9c792c403267d5d5 ([comment](https://github.com/pytorch/pytorch/pull/123496#issuecomment-2043251234))

### Weird (1)

- [Revert "Support all unsigned int sizes on unique (#123643)"](https://github.com/pytorch/pytorch/commit/d017645dc75ac76aa459e50be669e140465c451b)
  - Sorry for reverting your change, but it is failing lots of jobs with the new dtype https://hud.pytorch.org/pytorch/pytorch/commit/8aa08b8b9d1fab2a13dc5fbda74c553cb2a08729 ([comment](https://github.com/pytorch/pytorch/pull/123643#issuecomment-2047905094))
