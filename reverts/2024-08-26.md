# Week of 2024-08-26 to 2024-09-02 (48)

### GHFirst (30)

- [Revert "[Inductor] Allow customizing the padding format (#133939)"](https://github.com/pytorch/pytorch/commit/86e03a64e1e9570adfb574bc406cc5f3d8d90209)
  - sorry but this PR is causing issues with diff train imports reverting it for now but it can be merged back in as-is ([comment](https://github.com/pytorch/pytorch/pull/133939#issuecomment-2322635388))
- [Revert "[c10d] Remove Option for ProcessGroup and Expose backend Options to reflect the correct code structure (#132931)"](https://github.com/pytorch/pytorch/commit/351ba3e67c9dba6a1b448b0df4b9500ada1416d6)
  - This PR is breaking builds internally due to the removal of ProcessGroup::Options ([comment](https://github.com/pytorch/pytorch/pull/132931#issuecomment-2321862402))
- [Revert "[dynamo] simplify implementation for `functools.reduce` (#133778)"](https://github.com/pytorch/pytorch/commit/c6ecf57dd26a8f64de9e87e0fcc3d836ac32cc62)
  - This is still failing internally with the same error about 'Graph break due to unsupported builtin _functools.reduce' ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2321787968))
- [Revert "[dynamo] simplify implementation for `builtins.sum` (#133779)"](https://github.com/pytorch/pytorch/commit/7a85c488a8f3bdd78a986cc659e5f377bb4573d6)
  - This is still failing internally with the same error about 'Graph break due to unsupported builtin _functools.reduce' ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2321787968))
- [Revert "[dynamo][itertools] refactor `itertools.chain` and `itertools.chain.from_iterable` to use polyfills (#133864)"](https://github.com/pytorch/pytorch/commit/1ad08c7a5b83352e893d5e5f19a10de2afbf9086)
  - This is still failing internally with the same error about 'Graph break due to unsupported builtin _functools.reduce' ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2321787968))
- [Revert "[dynamo] refactor `builtins.enumerate` to use polyfill (#133894)"](https://github.com/pytorch/pytorch/commit/8aa44e14cf1e31de145ba53047faf6062e9ed338)
  - This is still failing internally with the same error about 'Graph break due to unsupported builtin _functools.reduce' ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2321787968))
- [Revert "[dynamo][itertools] refactor `itertools.islice` to use polyfill (#133876)"](https://github.com/pytorch/pytorch/commit/10c31e96dfdd2d2ebcdcd6bed01de4e0defe67cc)
  - This is still failing internally with the same error about 'Graph break due to unsupported builtin _functools.reduce' ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2321787968))
- [Revert "[reland][dtensor][MTPG] make sharding prop lru cache not shared among threads (#134509)"](https://github.com/pytorch/pytorch/commit/ab646cd8059b9309021fdb8b1356416507d12262)
  - Sorry but this fails internally. For details see D61953754 ([comment](https://github.com/pytorch/pytorch/pull/134509#issuecomment-2318323161))
- [Revert "[dynamo] simplify polyfill registration for `builtins.all` and `builtins.any` (#133769)"](https://github.com/pytorch/pytorch/commit/4811dc3de9d461933c4f5cbffc12478f015cc5b9)
  - Sorry but this has been discovered to be causing a performance regression internally ([comment](https://github.com/pytorch/pytorch/pull/133769#issuecomment-2316620213))
- [Revert "[dynamo][itertools] support `itertools.tee` (#133771)"](https://github.com/pytorch/pytorch/commit/f65df5edae97652f2acf50c35cf7d61e3a860209)
  - Sorry, have to revert this in order to be able to revert https://github.com/pytorch/pytorch/pull/133769 ([comment](https://github.com/pytorch/pytorch/pull/133771#issuecomment-2316611158))
- [Revert "[dynamo] simplify implementation for `os.fspath` (#133801)"](https://github.com/pytorch/pytorch/commit/eaec9e80b80e5e6b372b183c1ea308bcc3e58f93)
  - Sorry, have to revert this in order to be able to revert https://github.com/pytorch/pytorch/pull/133769 ([comment](https://github.com/pytorch/pytorch/pull/133771#issuecomment-2316611158))
- [Revert "[dynamo][exceptions] Use exception subclass whenever possible (#134610)"](https://github.com/pytorch/pytorch/commit/f0fceed432b0ea67eb9b5765133ec8e38cb6ea1c)
  - Sorry, I had to revert this in order to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/134610#issuecomment-2316568553))
- [Revert "[dynamo][dicts] Support hasattr on dicts (#134590)"](https://github.com/pytorch/pytorch/commit/67d7040fce79d37ebc8a5a42bcafaabd4fd8f58e)
  - Sorry, I had to revert this in order to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/134610#issuecomment-2316568553))
- [Revert "[raland][dynamo][exceptions] Support raise from None (#134621)"](https://github.com/pytorch/pytorch/commit/40cebde3bceb1073fe5458e8d61a491ee3e138c4)
  - Sorry, I had to revert this in order to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/134610#issuecomment-2316568553))
- [Revert "[dynamo] Graph break on FSDP flat_param inconsistent tensor and grad dtype (#134614)"](https://github.com/pytorch/pytorch/commit/c35d1f7b3aa4ec00017972c9d8ff1bb74436ba5c)
  - Sorry, I had to revert this in order to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/134610#issuecomment-2316568553))
- [Revert "[2nd try][Traceable FSDP2] Allow tracing through FSDP2 impl in trace_rules.py (#134539)"](https://github.com/pytorch/pytorch/commit/25531eb735c6c1ea38d60896fa74f561ab1f5174)
  - Sorry, I had to revert this in order to revert another PR ([comment](https://github.com/pytorch/pytorch/pull/134539#issuecomment-2316568257))
- [Revert "[2/N] Add flag to control which rank should perform NaN check (#134345)"](https://github.com/pytorch/pytorch/commit/66c33d59898632af038f8dedc110a46ba8d23e55)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/134345#issuecomment-2316133024))
- [Revert "[3/N] Set correct device to CUDA guards (#134357)"](https://github.com/pytorch/pytorch/commit/23e26b84af58e3ea3ca7acc27ec11bd0f0279ca5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/134357#issuecomment-2316121423))
- [Revert "hang dim hint constants off Dim (#134484)"](https://github.com/pytorch/pytorch/commit/13d40f6fc5908907701382f6be71f128ce3650b6)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/134484#issuecomment-2315749549))
- [Revert "Refactor caching device allocator utils (#130923)"](https://github.com/pytorch/pytorch/commit/2c88a923a79dc6bbb6fa496f42f13b46ad1360c6)
  - Sorry but this appears to be causing internal tests to fail with errors like `error: no type named 'DeviceStats' in namespace 'xxx::xxx:xxxAllocator'; did you mean 'DeviceStatus'?` ([comment](https://github.com/pytorch/pytorch/pull/130923#issuecomment-2315730155))
- [Revert "Adding entry-point based support for out-of-tree rendezvous plugins (#132633)"](https://github.com/pytorch/pytorch/commit/d52aff3e736c1df143adc162f6b84e6e280dce9a)
  - Sorry but this is causing internal tests to fail with the error `ImportError: cannot import name '_register_out_of_tree_handlers' from 'torch.distributed.elastic.rendezvous.registry'` ([comment](https://github.com/pytorch/pytorch/pull/132633#issuecomment-2315716201))
- [Revert "[export] enumerate unsupported sympy.Functions (#134271)"](https://github.com/pytorch/pytorch/commit/141a9c720465287a7f3900f16be2330bc5e14b92)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/134271#issuecomment-2311353460))
- [Revert "[dynamo] Cache _dynamo.disable results (#134272)"](https://github.com/pytorch/pytorch/commit/42955e04f1c9a51391614965e6b3884a594de6df)
  - Fails internal tests ([comment](https://github.com/pytorch/pytorch/pull/134272#issuecomment-2310649115))
- [Revert "[dynamo][guards] De-dupe DUPLICATE_INPUT guard (#134354)"](https://github.com/pytorch/pytorch/commit/e94bdc78763838b04a5e6436240d43c2b0ca06a1)
  - Fails internal tests ([comment](https://github.com/pytorch/pytorch/pull/134272#issuecomment-2310649115))
- [Revert "[inductor]Let output or input_as_strided match exact strides  (#130956)"](https://github.com/pytorch/pytorch/commit/17e8a51ff2cc0f3e2b16e17c00913188404f951a)
  - sorry but this seems to cause internal tests to fail. Please see D61771533 for details ([comment](https://github.com/pytorch/pytorch/pull/130956#issuecomment-2310490049))
- [Revert "c10d/logging: add C10D_LOCK_GUARD (#134131)"](https://github.com/pytorch/pytorch/commit/1c4780e69a415b32bfb2ba262281c700465368d8)
  - Sorry but this causes formatting errors internally which make it fail to build. See D61759282 ([comment](https://github.com/pytorch/pytorch/pull/134131#issuecomment-2310455878))
- [Revert "[dynamo] simplify implementation for `functools.reduce` (#133778)"](https://github.com/pytorch/pytorch/commit/50e90d7203bcea282fe5482fd93d11519bbfefe4)
  - Sorry, but this breaks internal tests because of using functools ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2310445169))
- [Revert "[dynamo] simplify implementation for `builtins.sum` (#133779)"](https://github.com/pytorch/pytorch/commit/472c7cf962f95d7338bbbfe03fe1140b6ffbe909)
  - Sorry, but this breaks internal tests because of using functools ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2310445169))
- [Revert "[dynamo][itertools] support `itertools.tee` (#133771)"](https://github.com/pytorch/pytorch/commit/3d7f3f6a555b95288bccd8429275594d9aedc350)
  - Sorry, but this breaks internal tests because of using functools ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2310445169))
- [Revert "[dynamo] simplify implementation for `os.fspath` (#133801)"](https://github.com/pytorch/pytorch/commit/e1fc4362fb0e8999660ebab64726301eec05526d)
  - Sorry, but this breaks internal tests because of using functools ([comment](https://github.com/pytorch/pytorch/pull/133778#issuecomment-2310445169))

### Not through pytorchbot (4)

- [Back out "[pytorch][PR] [export] Schematize nn_module_stack serialization" (#134628)](https://github.com/pytorch/pytorch/commit/71d0eff6e762cd558c6512b91d29153ca0658e58)
- [Back out "[dynamo][exception] Support raise exception from None (#134028)" (#134513)](https://github.com/pytorch/pytorch/commit/de57a6e806e990fa1b6914ca2b4aabb91bc7bc81)
- [Back out "[Traceable FSDPS] Allow tracing through FSDP2 impl in trace_rules.py (#133532)" (#134478)](https://github.com/pytorch/pytorch/commit/43bbd781f27e121820bd981fdeac37759eb8855d)
- [Revert "[CD] fix xpu nightly wheel test env (#134395)" (#134461)](https://github.com/pytorch/pytorch/commit/be96ccf77c460efc85d281efd501601368d90b55)

### No Signal (11)

- [Revert "[ROCm] remove triton-rocm commit pin and merge pins with triton.txt (#133438)"](https://github.com/pytorch/pytorch/commit/a1ba8e61d1b93f2116c5cccf1ce4b74a3fdaead4)
  - This still breaks linux binary builds. Added the appropriate labels to ensure tests can pass. See [GH job link](https://github.com/pytorch/pytorch/actions/runs/10626427003/job/29460479554) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/5e8bf29148a590318f678620f84be8f4d5ffff5c) ([comment](https://github.com/pytorch/pytorch/pull/133438#issuecomment-2322246198))
- [Revert "Add MaskedTensor support to *_like API (#128637)"](https://github.com/pytorch/pytorch/commit/503c0dd923fa1f96621f2bfad1772ef58a44c08f)
  - Actually, seems like it was this commit that introduced the failure: test_maskedtensor.py::TestOperatorsCUDA::test_like_empty_like_layout1_cuda_bool [GH job link](https://github.com/pytorch/pytorch/actions/runs/10604690725/job/29392898277) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/b6e51711a0ea6174806e75ab6e208d2d910b45f5) ([comment](https://github.com/pytorch/pytorch/pull/128637#issuecomment-2316554188))
- [Revert "Add torch.serialization.skip_data context manager (#134504)"](https://github.com/pytorch/pytorch/commit/128544399441fa0ab32f2a95237212c911da5078)
  - This is breaking Windows docs tests due to NamedTemporaryFile on Windows not working well ([comment](https://github.com/pytorch/pytorch/pull/134504#issuecomment-2316543901))
- [Revert "Add MaskedTensor passthrough: unfold, F.Unfold, F.Fold, stack (#125262)"](https://github.com/pytorch/pytorch/commit/f997b2b8e6f8f394961b96c4c4161d9643311e52)
  - Hi, this PR appears to be calling maskedtensor tests to fail on main. Please rebase your changes onto the latest trunk build to repro the failure. test_maskedtensor.py::TestOperatorsCUDA::test_like_empty_like_layout1_cuda_bool [GH job link](https://github.com/pytorch/pytorch/actions/runs/10604716811/job/29393256312) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/f685018ea9d08f98cbd7106028db134f967f74d3) ([comment](https://github.com/pytorch/pytorch/pull/125262#issuecomment-2316387447))
- [Revert "[dynamo] Cache _dynamo.disable results (#134272)"](https://github.com/pytorch/pytorch/commit/7b3da5f297521415cea977173d360a32bc06f730)
  - Peak mem increase detected internally ([comment](https://github.com/pytorch/pytorch/pull/134272#issuecomment-2316308170))
- [Revert "[dynamo][dicts] Support hasattr on dicts (#134590)"](https://github.com/pytorch/pytorch/commit/30094bedbc20cccef1fc6fa1ce4a74b8530be4f9)
  - causing trunk CI failures ([comment](https://github.com/pytorch/pytorch/pull/134590#issuecomment-2313705582))
- [Revert "fix stuck floordiv (#134150)"](https://github.com/pytorch/pytorch/commit/5b392d22c6e4e934464e90b4cf1d7861cd6861f2)
  - compile time regression internal ([comment](https://github.com/pytorch/pytorch/pull/134150#issuecomment-2313230404))
- [Revert "[FlexAttention] Create new variables for the subgraphs (#134507)"](https://github.com/pytorch/pytorch/commit/3418708abfc21d190a4658c477bfb2f42ac107f6)
  - Broke lint due to too long line ([comment](https://github.com/pytorch/pytorch/pull/134507#issuecomment-2312505955))
- [Revert "[FlexAttention] Remove unused code (#134511)"](https://github.com/pytorch/pytorch/commit/87a3f664e19b2577252316b028b324ba8370a681)
  - Broke lint due to too long line ([comment](https://github.com/pytorch/pytorch/pull/134507#issuecomment-2312505955))
- [Revert "[FlexAttention] Fix Sparse block multiple to ceildiv instead for floor div (#134538)"](https://github.com/pytorch/pytorch/commit/3e10a1eb5ae4f2e3b16f4029b993fcda7400a772)
  - Broke lint due to too long line ([comment](https://github.com/pytorch/pytorch/pull/134507#issuecomment-2312505955))
- [Revert "Allow mp.start_processes to create processes in parallel (#133707)"](https://github.com/pytorch/pytorch/commit/adcce538b78f11c644901c74a0385288f63d89e5)
  - sorry but trunk has been consistently broken since this PR was merged. See: [GH job link](https://github.com/pytorch/pytorch/actions/runs/10529617600/job/29191757055) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/3546628a2a167ace6060737eeccf8ee8fd87ddc0) ([comment](https://github.com/pytorch/pytorch/pull/133707#issuecomment-2310709523))

### Weird (3)

- [Revert "[1/N] Move NaN check onto NCCL stream (#134300)"](https://github.com/pytorch/pytorch/commit/cbf5ba1e970fc06680cc1bca82c38cffb6baed45)
  - This is breaking builds of MTIA ([comment](https://github.com/pytorch/pytorch/pull/134300#issuecomment-2316559704))
- [Revert "[2/N] Add flag to control which rank should perform NaN check (#134345)"](https://github.com/pytorch/pytorch/commit/33d0c11b26f03e2cb71ef18171724739b0897958)
  - This is breaking builds of MTIA ([comment](https://github.com/pytorch/pytorch/pull/134300#issuecomment-2316559704))
- [Revert "[3/N] Set correct device to CUDA guards (#134357)"](https://github.com/pytorch/pytorch/commit/43dc17fd0072693d497e82a4a4f71acaff293578)
  - This is breaking builds of MTIA ([comment](https://github.com/pytorch/pytorch/pull/134300#issuecomment-2316559704))
