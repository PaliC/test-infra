# Week of 2024-02-05 to 2024-02-12 (23)

### GHFirst (10)

- [Revert "[Dynamo][16/N] Move skipfiles to trace_rules.py (#119432)"](https://github.com/pytorch/pytorch/commit/eff93fbd86ab33b291a8b8881ad41a46a540839d)
  - Breaks internal tests ([comment](https://github.com/pytorch/pytorch/pull/119432#issuecomment-1936122795))
- [Revert "Add FakeTensor support to torch._utils._rebuild_tensor (#108186)"](https://github.com/pytorch/pytorch/commit/458e83b5b3bfe55a7384f5e201bc3663592b402a)
  - Reverted Internally ([comment](https://github.com/pytorch/pytorch/pull/108186#issuecomment-1935310344))
- [Revert "Fix estimate_nccl_collective_runtime (#118986)"](https://github.com/pytorch/pytorch/commit/7315ec7505401476c80a2a7fb34046f1f22682f2)
  - Breaks internal tests ([comment](https://github.com/pytorch/pytorch/pull/118986#issuecomment-1934680463))
- [Revert "[quant][pt2e][bc-breaking] Remove fold_quantize flag (#118701)"](https://github.com/pytorch/pytorch/commit/81abc2b2494ab7d48394b63d528eb5dddfa9d3d5)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/118701#issuecomment-1932866964))
- [Revert "Fix deadlock in ExecutionTraceObserver (#119242)"](https://github.com/pytorch/pytorch/commit/d85631b721268448bf4791c595b2ca08fd65bf06)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/119242#issuecomment-1931445631))
- [Revert "Add meta registration for _foreach_norm (#118604)"](https://github.com/pytorch/pytorch/commit/dea15c9fdcd0d8cf09612ea2861541d1e743d5e5)
  - Breaks internal tests ([comment](https://github.com/pytorch/pytorch/pull/118604#issuecomment-1930849491))
- [Revert "[c10d] PGNCCL refactor part 1: adds assert size==1 (#119099)"](https://github.com/pytorch/pytorch/commit/9d46fe603d9f4ec54e9f2bc3fec54814766b47f8)
  - Breaks internal tests ([comment](https://github.com/pytorch/pytorch/pull/119099#issuecomment-1930839754))
- [Revert "Add FakeTensor support to torch._utils._rebuild_tensor (#108186)"](https://github.com/pytorch/pytorch/commit/499040ac32b12d4250161e0bced8412402d08833)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/108186#issuecomment-1929978008))
- [Revert "[inductor] make multi-kernel work with cpp-wrapper (#117813)"](https://github.com/pytorch/pytorch/commit/b964a1222cef0af3aebe04e25714aa08c97a6cd3)
  - Failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/117813#issuecomment-1927877102))
- [Revert "refactor lazy init to device-agnostic (#118846)"](https://github.com/pytorch/pytorch/commit/ab613a4019db831855a0783c8387275ba7e56500)
  - Failing, tests https://github.com/pytorch/torchdistx/blob/main/src/python/torchdistx/_C/fake.cc#L11  ([comment](https://github.com/pytorch/pytorch/pull/118846#issuecomment-1927651305))

### Landrace (1)

- [Revert "[BE] Add dtypesIfMPS to ModuleInfo enabling float16 tests for MPS and remove all skipIfMPS for float64 (#119039)"](https://github.com/pytorch/pytorch/commit/c0164f2393bca5e9c5b49a193e170d64656d71df)
  - Sorry for reverting your change but it is failing MPS test in trunk https://hud.pytorch.org/pytorch/pytorch/commit/04d52d5399ad4abb8af9e8405be79e2a7f8b4c7a,  may be a landrace ([comment](https://github.com/pytorch/pytorch/pull/119039#issuecomment-1928595240))

### Not through pytorchbot (1)

- [Revert "add Half support for flash attention on CPU (#118368)" (#119204)](https://github.com/pytorch/pytorch/commit/f48183511546a0be9936476a86d61fea19f23555)

### No Signal (7)

- [Revert "[aot_inductor] move CudaWrapperCodeGen into a separate file (#119448)"](https://github.com/pytorch/pytorch/commit/3ab08946d5052eaeda11d683d6a58e801a032755)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/119448#issuecomment-1937345167))
- [Revert "[aot_inductor] move CppWrapperCodeGen into a separate file (#119491)"](https://github.com/pytorch/pytorch/commit/d8e319a961bb872027f0abdc413d6beb7502ac9b)
  - Reverted as a dependency for #119448 ([comment](https://github.com/pytorch/pytorch/pull/119491#issuecomment-1937344548))
- [Revert "make flash_attn_bw impl correct w.r.t. meta when k and v have different strides (#119500)"](https://github.com/pytorch/pytorch/commit/34db6f1b13206d0b5cc3297e4a92dd0c4b8aea45)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/119500#issuecomment-1937003082))
- [Revert "[dynamo] Improve support for backwards hooks (#119525)"](https://github.com/pytorch/pytorch/commit/25a0fa6d139c324cbdac31b96725de57ab419089)
  - broke test_autograd.py::TestAutograd::test_post_accumulate_grad_hook_gets_cleaned_up on dynamo https://github.com/pytorch/pytorch/actions/runs/7847212828/job/21416215820 https://hud.pytorch.org/pytorch/pytorch/commit/b1f4b2a63c038f0090886d7d213825f39c283ea5.  The failure exists on the PR as well, but got masked by the other test.  Putting this as no signal? ([comment](https://github.com/pytorch/pytorch/pull/119525#issuecomment-1936447169))
- [Revert "Fix jagged NT softmax semantics (#119459)"](https://github.com/pytorch/pytorch/commit/8994f2367ddb1d85351ad0b40a4de73d83f8c63f)
  - broke dynamo, see https://github.com/pytorch/pytorch/actions/runs/7835402753/job/21386634602 ([comment](https://github.com/pytorch/pytorch/pull/119459#issuecomment-1935246413))
- [Revert "Add lowering for logcumsumexp (#118753)"](https://github.com/pytorch/pytorch/commit/a8d1645f156c6e8833c11eb1fc32ce45147575ac)
  - broke ROCm CI, but not seen until trunk job ([comment](https://github.com/pytorch/pytorch/pull/118753#issuecomment-1935074235))
- [Revert "Update scatter_reduce_ test with parallel backend check (#118708)"](https://github.com/pytorch/pytorch/commit/e47f571da7368092d3fe9f10d899c00543b414fb)
  - Test Case still fail ([comment](https://github.com/pytorch/pytorch/pull/118708#issuecomment-1928767568))

### Weird (4)

- [Revert "Fix delete branches (#119399)"](https://github.com/pytorch/pytorch/commit/c6f39740c78e2280cba0ea339b5255bed25427a5)
  - has a bug ([comment](https://github.com/pytorch/pytorch/pull/119399#issuecomment-1936291560))
- [Revert "Add cpp stack traces to our own reruns (#119408)"](https://github.com/pytorch/pytorch/commit/8182fce76913f70822158f1c394be217122e66f6)
  - Looks like it introduced intermittent crashes see https://github.com/pytorch/pytorch/actions/runs/7823402867/job/21344456540 for example, testing the theory ([comment](https://github.com/pytorch/pytorch/pull/119408#issuecomment-1934589057))
- [Revert "[Inductor] GEMM shape padding improvements (#118522)"](https://github.com/pytorch/pytorch/commit/088d538a8d37621e9cacbc37f7120b94874e2481)
  - regresses HF ~4/5% ([comment](https://github.com/pytorch/pytorch/pull/118522#issuecomment-1932557670))
- [Revert "Remove extra graph breaks (#118987)"](https://github.com/pytorch/pytorch/commit/966db82c9dbb64eac229315d785cd283dd8faec4)
  - reverting because it causes regression ([comment](https://github.com/pytorch/pytorch/pull/118987#issuecomment-1928224447))
