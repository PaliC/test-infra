# Week of 2024-05-20 to 2024-05-27 (32)

### GHFirst (10)

- [Revert "[AOTI] Add more fallback ops (#126720)"](https://github.com/pytorch/pytorch/commit/47c976b904ee77fb564804547bdcfa9411cd9a72)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126720#issuecomment-2129011751))
- [Revert "[AOTI] Fix an int array codegen issue (#126801)"](https://github.com/pytorch/pytorch/commit/f749c5def8e8cbc2cb2b788afe0eb7b377e87886)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126720#issuecomment-2129011751))
- [Revert "[AOTI] Disable stack allocation for OSS (#125732)"](https://github.com/pytorch/pytorch/commit/fd9cdeed192fd8ab00035716d42f2e8feef3373b)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126720#issuecomment-2129011751))
- [Revert "Fix flexattention not realizing inputs before lowering (also refactored runtime estimation) (#126615)"](https://github.com/pytorch/pytorch/commit/8a4597980c2692b73f35fb3c7145eaeaf2273e77)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126615#issuecomment-2124169157))
- [Revert "Prevent partitioner from ever saving views (#126446)"](https://github.com/pytorch/pytorch/commit/0f37fd06d91283c013ff8bf5aa292c05b5cea691)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126615#issuecomment-2124169157))
- [Revert "Fix silu test for flexattention (#126641)"](https://github.com/pytorch/pytorch/commit/d2cbbdee310e1883ba22e99d34760550d0564eff)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126615#issuecomment-2124169157))
- [Revert "[Quant][PT2E] enable qlinear post op fusion for dynamic quant & qat (#122667)"](https://github.com/pytorch/pytorch/commit/980f5ac0499447cd6d39b6241ae30ae30937a3e5)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/122667#issuecomment-2122642317))
- [Revert "c10d: add Collectives abstraction (#125978)"](https://github.com/pytorch/pytorch/commit/d9c3485146913324ab4b3e211d2a4517e138f4af)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/125978#issuecomment-2119858015))
- [Revert "Add symbolic_shape_specialization structured trace (#126450)"](https://github.com/pytorch/pytorch/commit/53f73cdeb6fe53155f91064ca15b722e80dec2f3)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126450#issuecomment-2119798075))
- [Revert "[inductor] Load python modules using importlib (#126454)"](https://github.com/pytorch/pytorch/commit/5ad2f100340c64123db607824f70119c8fe2b38a)
  - Break internal build ([comment](https://github.com/pytorch/pytorch/pull/126454#issuecomment-2119771267))

### Ignored Signal (6)

- [Revert "reset dynamo cache before each test (#126586)"](https://github.com/pytorch/pytorch/commit/12d11fe4e533abc9c48f5ea869831a9698313665)
  - Broke tons of tests, see https://hud.pytorch.org/pytorch/pytorch/commit/bd24991f461476036d6ba20fed92651c7e46ef7c  ([comment](https://github.com/pytorch/pytorch/pull/126586#issuecomment-2131365576))
- [Revert "[inductor][cpp] GEMM template (infra and fp32) (#124021)"](https://github.com/pytorch/pytorch/commit/25b8dbc3e4f1e3df35c4fd1864e0ef2a73c12d2f)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126568331))
- [Revert "[inductor][cpp] epilogue support for gemm template (#126019)"](https://github.com/pytorch/pytorch/commit/45784cd2294cad828daacdd66ec7b82e22c9cea1)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126568331))
- [Revert "[inductor][cpp] bf16/fp16 gemm template computed with fp32 w/o epilogue fusion (#126068)"](https://github.com/pytorch/pytorch/commit/926327e8fc887ff9eaf1f9dacf15ff8489a51e4b)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126568331))
- [Revert "[inductor][cpp] support bf16/fp16 gemm template epilogue fusion (#126545)"](https://github.com/pytorch/pytorch/commit/30c9ca089912fb452b3e1d5dd7a8be92139344e8)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126568331))
- [Revert "[pipelining] Add pipeline stage test (#126721)"](https://github.com/pytorch/pytorch/commit/e363a8a222d286e7b5da71a931ebbfaa729346db)
  - The test_public_bindings failure is real, Dr CI is wrong since it was also broken on trunk for a different reason ([comment](https://github.com/pytorch/pytorch/pull/126721#issuecomment-2121725408))

### Landrace (4)

- [Revert "[AOTI] support freezing for MKLDNN (#124350)"](https://github.com/pytorch/pytorch/commit/5ae9daa4a2812c2ec4270f1002be932cf05046ee)
  - Seems to have broken inductor/test_aot_inductor.py::AOTInductorTestNonABICompatibleCpu::test_freezing_non_abi_compatible_cpu https://hud.pytorch.org/pytorch/pytorch/commit/654afb6f3ae3ddbd926a753f9af95a6f6e22131c https://github.com/pytorch/pytorch/actions/runs/9224838183/job/25382780192 ([comment](https://github.com/pytorch/pytorch/pull/124350#issuecomment-2129889809))
- [Revert "[inductor][cpp] GEMM template (infra and fp32) (#124021)"](https://github.com/pytorch/pytorch/commit/4f14282e350a6c9a0a280083c286f7d672fa3ebd)
  - Sorry for reverting your change but I think it has a land race and failing in trunk https://hud.pytorch.org/pytorch/pytorch/commit/2ac33a9f663269e6060246337c776a20c3b7c858 ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126016522))
- [Revert "[inductor][cpp] epilogue support for gemm template (#126019)"](https://github.com/pytorch/pytorch/commit/657d39e44c9ba8f996796a7d50ee8c6fa4fc1756)
  - Sorry for reverting your change but I think it has a land race and failing in trunk https://hud.pytorch.org/pytorch/pytorch/commit/2ac33a9f663269e6060246337c776a20c3b7c858 ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126016522))
- [Revert "[inductor][cpp] bf16/fp16 gemm template computed with fp32 w/o epilogue fusion (#126068)"](https://github.com/pytorch/pytorch/commit/205f08140eefd9a1bc85e0c3eb7b553fc36fba5a)
  - Sorry for reverting your change but I think it has a land race and failing in trunk https://hud.pytorch.org/pytorch/pytorch/commit/2ac33a9f663269e6060246337c776a20c3b7c858 ([comment](https://github.com/pytorch/pytorch/pull/124021#issuecomment-2126016522))

### Not through pytorchbot (2)

- [Revert "Skip test_memory_format_nn_BatchNorm2d in inductor (#125970)" (#126594)](https://github.com/pytorch/pytorch/commit/db9c6aeec60dc023f8b3edd82ea24ebc0b3108c6)
- [Revert "Skip test_memory_format_nn_BatchNorm2d in inductor (#125970)" (#126594)](https://github.com/pytorch/pytorch/commit/ce6e36bf8b524c3f4b07605c5b3af2b7d5ba8fd9)

### No Signal (8)

- [Revert "[pipelining] Add grad test for interleaved schedules (#126931)"](https://github.com/pytorch/pytorch/commit/dfabae5b8997c5a3eb2c474d9b2b9e181f5d0f5c)
  - newly added test fails distributed/pipelining/test_schedule.py::ScheduleTest::test_grad_with_manual_interleaved_ScheduleClass0 https://hud.pytorch.org/pytorch/pytorch/commit/abf6d4e6bc1a9a0e08bfc2204560ca7858fa90cd https://github.com/pytorch/pytorch/actions/runs/9214413308/job/25352507591, pull workflow failed on startup on PR, so no distributed tests ran at all ([comment](https://github.com/pytorch/pytorch/pull/126931#issuecomment-2128228496))
- [Revert "Default XLA to use swap_tensors path in nn.Module._apply (#126814)"](https://github.com/pytorch/pytorch/commit/b36e390b6c1f4a1a2239ad1e857a2ffd12f49e43)
  - broke xla ci ([comment](https://github.com/pytorch/pytorch/pull/126814#issuecomment-2127719337))
- [Revert "Default meta device to use swap_tensors in nn.Module._apply  (.to_empty and .to('meta')) (#126819)"](https://github.com/pytorch/pytorch/commit/6a06d362961772e630f599285dd8d3e9c286114d)
  - broke xla ci ([comment](https://github.com/pytorch/pytorch/pull/126814#issuecomment-2127719337))
- [Revert "Introduce ProcessGroupCudaP2P (#122163)"](https://github.com/pytorch/pytorch/commit/1b29c16e5e143eb49a902d384b3d8a47dbe62a31)
  - This is breaking ROCm distributed CI on trunk ([comment](https://github.com/pytorch/pytorch/pull/122163#issuecomment-2127518473))
- [Revert "reset dynamo cache before each test (#126586)"](https://github.com/pytorch/pytorch/commit/2c90b992677a7ee700c8f3dd92b49e763474b004)
  - broke tests on inductor? test_modules.py::TestModuleCUDA::test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 https://hud.pytorch.org/pytorch/pytorch/commit/43f2f43eb3b6d8cbe8eb7f45acb50376092f1a16 https://github.com/pytorch/pytorch/actions/runs/9200644034/job/25308511495 ([comment](https://github.com/pytorch/pytorch/pull/126586#issuecomment-2126228689))
- [Revert "don't check memory format for empty tensors (#126593)"](https://github.com/pytorch/pytorch/commit/b1e214ceb19aa79c0fbfe3ee0e53d615bfa68801)
  - broke tests on inductor? test_modules.py::TestModuleCUDA::test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 https://hud.pytorch.org/pytorch/pytorch/commit/43f2f43eb3b6d8cbe8eb7f45acb50376092f1a16 https://github.com/pytorch/pytorch/actions/runs/9200644034/job/25308511495 ([comment](https://github.com/pytorch/pytorch/pull/126586#issuecomment-2126228689))
- [Revert "[TD] Upload names of failures to s3 for pytest cache (#126315)"](https://github.com/pytorch/pytorch/commit/8bca0847c2c2a80b5dfab6dd554c7dd68199a3b3)
  - broke inductor ([comment](https://github.com/pytorch/pytorch/pull/126315#issuecomment-2121133045))
- [Revert " Updated test_graph_optims and test_graph_scaling_fused_optimizers to use new OptimizerInfo infrastructure (#125127)"](https://github.com/pytorch/pytorch/commit/cb69c51b6f1d1803c8c2a01d26e3b3474917cf39)
  - Broken trunk ([comment](https://github.com/pytorch/pytorch/pull/125127#issuecomment-2120337584))

### Weird (2)

- [Revert "Lift jagged -> padded dense forward / backward kernels from fbgemm_gpu (#125946)"](https://github.com/pytorch/pytorch/commit/c34f8c7f91dfce2d4ac85e10660bcfbd70ab2406)
  - sorry the Dr CI fix hasn't been merged yet and its still failing https://hud.pytorch.org/pytorch/pytorch/commit/5e69e11d098a2cfccc8a59377c431e9c71cab9a8 https://github.com/pytorch/pytorch/actions/runs/9228887299/job/25393895252 ([comment](https://github.com/pytorch/pytorch/pull/125946#issuecomment-2130305958))
- [Revert "Lift jagged -> padded dense forward / backward kernels from fbgemm_gpu (#125946)"](https://github.com/pytorch/pytorch/commit/99a11efc8ad8de16ffd538971a03398f6343c9a7)
  - I think dr ci is wrong and the windows build failure is real https://hud.pytorch.org/pytorch/pytorch/commit/e2f081837f4276c1a6a37739bd28157f62004a06 https://github.com/pytorch/pytorch/actions/runs/9216826622/job/25357819877 ([comment](https://github.com/pytorch/pytorch/pull/125946#issuecomment-2128388126))
